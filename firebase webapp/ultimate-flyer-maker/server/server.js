// server/server.js

import fetch from "node-fetch"; // if Node < v18; else can omit and use global fetch
import express from "express";
import cors from "cors";
import multer from "multer";
import "./config/firebase.js";
import fs from "fs";
import sharp from "sharp";
import vision from "@google-cloud/vision";
import "dotenv/config"; // loads .env automatically
import "./config/googleAuth.js";  
import identifyProductRoute from "./routes/identifyProductRoute.js";


import { parseProductTitle } from "./parseTitleDeepSeek.js";
import searchByPhotoRoute from "./routes/searchByPhotoRoute.js";
import searchByImageRoute from "./routes/searchByImageRoute.js";
import productRoutes from "./routes/productRoutes.js";
import checkDuplicateImageRoute from "./routes/checkDuplicateImageRoute.js";
import { searchBrave, buildBraveQueryFromParsed } from "./services/braveSearchService.js";


// ---- SETUP ----
const app = express();

// âœ… 1) Body parsers FIRST (for JSON APIs like /api/products)
app.use(express.json({ limit: "10mb" }));
app.use(express.urlencoded({ extended: true, limit: "10mb" }));

// âœ… 2) CORS BEFORE ANY ROUTES (important for login + React)
app.use(
  cors({
    origin: "http://localhost:3000", // your React dev server
    methods: ["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allowedHeaders: ["Content-Type", "Authorization"],
    credentials: true, // allow cookies / auth headers for login
  })
);
app.use("/api/check-duplicate-image", checkDuplicateImageRoute);


// âœ… 3) Multer for file upload (used by /analyze)
const upload = multer({ dest: "uploads/" });

// ---- GOOGLE VISION ----
const client = new vision.ImageAnnotatorClient();

// ---- DEEPSEEK ----
const DEEPSEEK_API_KEY = process.env.DEEPSEEK_API_KEY;

async function refineWithDeepSeek(ocrText) {
  try {
    const response = await fetch("https://api.deepseek.com/v1/chat/completions", {
      method: "POST",
      headers: {
        Authorization: `Bearer ${DEEPSEEK_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        model: "deepseek-chat",
        messages: [
          {
            role: "system",
            content: `
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å•†å“æ ‡é¢˜æå–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡ï¼š
        1) ä»Ž OCR æ–‡å­—ä¸­æå–æœ€èƒ½ä»£è¡¨è¯¥å•†å“çš„ä¸­æ–‡åç§°
        2) æå–å¯¹åº”çš„è‹±æ–‡åç§°ï¼ˆåªèƒ½æ ¹æ® OCR ä¸­å‡ºçŽ°æˆ–èƒ½å¤Ÿç›´æŽ¥ç¿»è¯‘çš„å†…å®¹ï¼‰
        3) å¦‚æœ‰é‡é‡ä¿¡æ¯ï¼Œä¹Ÿéœ€è¦æå–ï¼›æ²¡æœ‰åˆ™ç•™ç©º

        **ä¸¥æ ¼è¦æ±‚ï¼š**
        - åªèƒ½åŸºäºŽ OCR ä¸­çœŸå®žå‡ºçŽ°çš„ä¿¡æ¯ï¼Œä¸å¾—å‘æŒ¥ï¼Œä¸å¾—çŒœæµ‹ï¼Œä¸å¾—åˆ›é€ ä¸å­˜åœ¨çš„å“ç‰Œæˆ–åç§°ã€‚
        - å¦‚æžœ OCR æœªå‡ºçŽ°æ˜Žç¡®çš„å•†å“åç§°ï¼ˆä¾‹å¦‚â€œé¥ºå­çš®â€â€œè±†è…â€â€œé¢æ¡â€ç­‰ï¼‰ï¼Œå…è®¸è¾“å‡ºä¸€ä¸ªåˆç†çš„é€šç”¨åç§°ï¼Œä¾‹å¦‚â€œé£Ÿå“â€æˆ–â€œOCR Unclearâ€ï¼Œä½†ä¸è¦è¾“å‡ºâ€œäº§å“ / Productâ€è¿™ç±»æ— æ„ä¹‰çš„è¯ã€‚
        - ä¸å¾—å‚è€ƒç¤ºä¾‹ï¼Œä¸å¾—æ¨¡ä»¿æ ¼å¼ä¸­çš„è¯è¯­ã€‚
        - ä¼˜å…ˆä½¿ç”¨å‡ºçŽ°é¢‘çŽ‡æœ€é«˜ã€å­—ä½“æœ€å¤§çš„ä¸­æ–‡å†…å®¹ã€‚
        - å¦‚æžœåŒæ—¶å‡ºçŽ°ä¸­è‹±æ–‡å­—ï¼Œå¿…é¡»ä¿æŒè¯­ä¹‰ä¸€è‡´ã€‚
        - ä¸å…è®¸è¾“å‡ºç¤ºä¾‹ä¸­çš„è–¯ç‰‡ã€Lays ç­‰è¯ã€‚

        è¾“å‡ºæ ¼å¼ï¼ˆå…±ä¸‰è¡Œï¼Œä¸å¤šä¸å°‘ï¼‰ï¼š
        <ä¸­æ–‡æ ‡é¢˜>
        <English Title>
        <weight or empty>
            `,
          },
          {
            role: "user",
            content: `
        ä»¥ä¸‹æ˜¯OCRè¯†åˆ«æ–‡å­—ï¼Œè¯·åˆ†æžå…¶ä¸­çœŸå®žå‡ºçŽ°çš„å¯ä½œä¸ºå•†å“åç§°çš„å†…å®¹ï¼š

        --------------------
        ${ocrText}
        --------------------

        å¦‚æžœ OCR æ–‡æœ¬ä¸­æ²¡æœ‰æ˜Žæ˜¾çš„å•†å“åç§°ï¼Œè¯·è¾“å‡ºæœ€åˆç†çš„é€šç”¨æè¿°ï¼Œå¦‚ï¼š
        é£Ÿå“
        Food Item

        è¯·æŒ‰ä¸‰è¡Œæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚
            `,
          },
        ],

        temperature: 0.1,   // more deterministic
        max_tokens: 150,
      }),
    });

    if (!response.ok) {
      const text = await response.text();
      console.error("âŒ DeepSeek API error:", response.status, text);
      return null;
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content?.trim() || "";

    const lines = content.split(/\n+/).map(s => s.trim()).filter(Boolean);
    const joined = lines.join(" ");

    const weightMatch = joined.match(/(\d+(?:\.\d+)?\s?(g|kg|å…‹|æ¯«å‡|ml|oz|l))/i);
    const weight = weightMatch ? weightMatch[1].trim() : "";

    return {
      title_zh: lines[0] || "",
      title_en: lines[1] || "",
      weight,
      raw: content
    };
  } catch (e) {
    console.error("âš ï¸ DeepSeek call failed:", e);
    return null;
  }
}



// ðŸˆ¶ Drop English transliterations if Chinese dupes exist
const dropEnglishDupes = (lines) => {
  const chinese = lines.filter((l) => /[\u4e00-\u9fa5]/.test(l));
  return lines.filter((l) => {
    if (!/[A-Za-z]/.test(l)) return true;
    const normalized = l.replace(/[A-Za-z]/g, "").trim();
    return !chinese.some((c) => c.includes(normalized.slice(0, 2)));
  });
};

// ðŸ§  Refined language-weighted parser with product-core clustering
function parseProductText(texts) {
  if (!texts || !texts.length)
    return { title: "", note: "no text", confidence: 0 };

  const normalize = (s) =>
    s.replace(/[^\u4e00-\u9fa5A-Za-z0-9\s]/g, " ")
      .replace(/\s+/g, " ")
      .trim();
  let lines = Array.from(new Set(texts.map(normalize).filter(Boolean)));

  lines = dropEnglishDupes(lines);

  const joined = lines.join(" ");
  const weightMatch = joined.match(/(\d+(?:\.\d+)?\s?(g|kg|å…‹|æ¯«å‡|ml|oz|l))/i);
  const weight = weightMatch ? weightMatch[1].trim() : "";

  // --- 1ï¸âƒ£ find repeating Chinese substrings ---
  const words = joined.match(/[\u4e00-\u9fa5]{2,4}/g) || [];
  const freq = {};
  for (const w of words) freq[w] = (freq[w] || 0) + 1;
  const sorted = Object.entries(freq)
    .filter(([w, n]) => n >= 2)
    .sort((a, b) => b[1] - a[1]);
  const productCore = sorted.length ? sorted[0][0] : "";

  // --- 2ï¸âƒ£ find best product line containing that core ---
  let productLine = "";
  if (productCore) {
    const candidates = lines.filter((l) => l.includes(productCore));
    productLine =
      candidates.sort((a, b) => b.length - a.length)[0] || productCore;
  } else {
    productLine =
      lines.find((l) => /[\u4e00-\u9fa5]{3,}/.test(l)) ||
      lines.find((l) => /[\u4e00-\u9fa5]/.test(l)) ||
      lines[0];
  }

  // --- 3ï¸âƒ£ choose brand: short Chinese line near productLine ---
  const productIndex = lines.indexOf(productLine);
  let brand = "";
  if (productIndex > 0) {
    const before = lines
      .slice(Math.max(0, productIndex - 3), productIndex)
      .filter((l) => /[\u4e00-\u9fa5]{2,4}/.test(l));
    brand =
      before.sort((a, b) => a.length - b.length)[0] ||
      lines.find((l) => /[\u4e00-\u9fa5]{2,4}/.test(l));
  }

  // --- 4ï¸âƒ£ detect series ---
  const series =
    (productCore && joined.split(productCore).length > 3) ||
    /(ç³»åˆ—)/.test(joined);
  if (series && !/ç³»åˆ—$/.test(productLine)) productLine += "ç³»åˆ—";

  // --- 5ï¸âƒ£ compose title ---
  const title = [brand, productLine, weight].filter(Boolean).join(" ").trim();

  return {
    brand,
    product: productLine,
    main: productLine,
    weight,
    title,
    confidence: 0.95,
    note: series
      ? "Repeated product pattern detected (series)."
      : "Core-word clustering applied.",
  };
}

// ----------------------------
// âœ… API ROUTES (use CORS/body already set up)
// ----------------------------

// Health check
app.get("/", (req, res) => {
  res.send("Backend is live âœ…");
});

// Product + image search routes
app.use("/api/search-by-image", searchByImageRoute);
app.use("/api/search-by-photo", searchByPhotoRoute);
app.use("/api/products", productRoutes); // âœ… only once now
app.use("/api/identify-product", identifyProductRoute);


// ----------------------------
// /analyze ROUTE
// ----------------------------
app.post("/analyze", upload.single("image"), async (req, res) => {
  const filePath = req.file.path;
  console.log("ðŸ“¸ File uploaded:", filePath);

  try {
    console.log("ðŸ§  Running object localization...");
    const [localizeResult] = await client.objectLocalization(filePath);
    const objects = localizeResult.localizedObjectAnnotations || [];
    console.log("ðŸ” Object count:", objects.length);

    let texts = [];
    let croppedBase64 = "";
    let detectedObject = "none";

    const visualSearchPath = fs.existsSync(`${filePath}-crop.jpg`)
      ? `${filePath}-crop.jpg`
      : filePath;

    // --- Case A: no object localization ---
if (!objects.length) {
  console.log("âš ï¸ No objects detected. Running OCR on full image...");
  const [textResult] = await client.textDetection(filePath);
  texts = textResult.textAnnotations?.map((t) => t.description) || [];

  const buffer = fs.readFileSync(filePath);
  croppedBase64 = buffer.toString("base64");

  // ðŸ” JOIN OCR TEXT INTO MULTI-LINE STRING
  const ocrText = texts.join("\n");

  // ðŸ“ PRINT FULL OCR TEXT FOR DEBUGGING
  console.log("===== ðŸ“ OCR RAW TEXT BEGIN =====");
  console.log(ocrText);
  console.log("===== ðŸ“ OCR RAW TEXT END =====");

  console.log("ðŸ§¾ OCR extracted text length:", ocrText.length);


  try {
      console.log("ðŸ¤– Calling DeepSeek parseTitleDeepSeek...");
      const aiTitle = await parseProductTitle(ocrText);

      const parsed = {
        title_ai: aiTitle,
        title_zh: "", // no structured split here, but keep fields for Brave
        title_en: "",
        weight: "",
        note: "Used DeepSeek AI parser due to no localized objects.",
      };

      // ðŸ§  Build Brave query from parsed / AI title / OCR text
      const braveQuery = buildBraveQueryFromParsed(parsed, aiTitle || ocrText);

      let braveResults = [];
      try {
        braveResults = await searchBrave(braveQuery, { count: 5 });
        console.log("ðŸ§­ Brave results (no-objects):", braveResults.length);
      } catch (e) {
        console.error("âš ï¸ Brave search failed in no-objects branch:", e);
      }

      fs.unlinkSync(filePath);
      return res.json({ detectedObject, texts, parsed, croppedBase64, braveResults });
    } catch (err) {
      console.error(
        "âŒ DeepSeek failed, falling back to local parser:",
        err.message
      );
      const parsed = parseProductText(texts);

      // ðŸ§  Build Brave query from local parsed result + OCR
      const braveQuery = buildBraveQueryFromParsed(parsed, ocrText);

      let braveResults = [];
      try {
        braveResults = await searchBrave(braveQuery, { count: 5 });
        console.log("ðŸ§­ Brave results (fallback no-objects):", braveResults.length);
      } catch (e) {
        console.error("âš ï¸ Brave search failed in fallback no-objects branch:", e);
      }

      fs.unlinkSync(filePath);
      return res.json({ detectedObject, texts, parsed, croppedBase64, braveResults });
    }
  }
   else {
      // --- Case B: crop the most central/large object, but rank by OCR density ---
      const { width, height } = await sharp(filePath).metadata();
      const centerX = width / 2;
      const centerY = height / 2;

      const scored = objects.map((obj) => {
        const v = obj.boundingPoly.normalizedVertices;
        const xs = v.map((v) => v.x * width);
        const ys = v.map((v) => v.y * height);
        const left = Math.min(...xs);
        const right = Math.max(...xs);
        const top = Math.min(...ys);
        const bottom = Math.max(...ys);
        const area = (right - left) * (bottom - top);
        const distance = Math.sqrt(
          (centerX - (left + right) / 2) ** 2 +
            (centerY - (top + bottom) / 2) ** 2
        );
        const score = area / (distance + 1);
        return { obj, left, top, right, bottom, score };
      });

      console.log("ðŸ”Ž Scoring objects by OCR text density...");
      for (const candidate of scored) {
        const cropPath = `${filePath}-${candidate.obj.name}.jpg`;

        await sharp(filePath)
          .extract({
            left: Math.max(0, Math.floor(candidate.left)),
            top: Math.max(0, Math.floor(candidate.top)),
            width: Math.floor(candidate.right - candidate.left),
            height: Math.floor(candidate.bottom - candidate.top),
          })
          .resize(600)
          .toFile(cropPath);

        const [textResult] = await client.textDetection(cropPath);
        const textCount = textResult.textAnnotations?.length || 0;
        candidate.textCount = textCount;

        fs.unlinkSync(cropPath);
      }

      // pick the object with most text annotations
      scored.sort((a, b) => b.textCount - a.textCount);
      const best = scored[0];
      detectedObject = best.obj.name;
      console.log(
        `ðŸŽ¯ Selected object: ${detectedObject} (textCount=${best.textCount})`
      );

      const cropPath = `${filePath}-crop.jpg`;
      await sharp(filePath)
        .extract({
          left: Math.max(0, Math.floor(best.left)),
          top: Math.max(0, Math.floor(best.top)),
          width: Math.floor(best.right - best.left),
          height: Math.floor(best.bottom - best.top),
        })
        .toFile(cropPath);

      const [textResult] = await client.textDetection(cropPath);
      texts = textResult.textAnnotations?.map((t) => t.description) || [];
      // ðŸ” JOIN OCR TEXT INTO MULTI-LINE STRING
      const ocrText = texts.join("\n");

      // ðŸ“ PRINT FULL OCR TEXT FOR DEBUGGING
      console.log("===== ðŸ“ OCR RAW TEXT BEGIN =====");
      console.log(ocrText);
      console.log("===== ðŸ“ OCR RAW TEXT END =====");


      // fallback if cropped OCR is empty
      if (!texts.length) {
        console.log("âš ï¸ Cropped OCR empty. Fallback to full image OCR...");
        const [textResultFull] = await client.textDetection(filePath);
        texts = textResultFull.textAnnotations?.map((t) => t.description) || [];
        const buffer = fs.readFileSync(filePath);
        croppedBase64 = buffer.toString("base64");
      } else {
        const buffer = fs.readFileSync(cropPath);
        croppedBase64 = buffer.toString("base64");
        fs.unlinkSync(cropPath);
      }
    }

        // ---- 6ï¸âƒ£ Use local parser first ----
    const parsed = parseProductText(texts);
    console.log("âœ… Local parsed title:", parsed.title);

    // ---- 7ï¸âƒ£ Refine with DeepSeek ----
    const joinedText = texts.join(" ");
    const aiParsed = await refineWithDeepSeek(joinedText);

    if (aiParsed) {
      parsed.title_zh = aiParsed.title_zh || parsed.title || "";
      parsed.title_en = aiParsed.title_en || "";
      parsed.weight = aiParsed.weight || parsed.weight || "";
      parsed.raw_ai = aiParsed.raw || "";

      parsed.title_ai = [parsed.title_zh, parsed.title_en, parsed.weight]
        .filter(Boolean)
        .join("\n");

      parsed.note += " | DeepSeek bilingual refinement applied.";
      console.log("ðŸ¤– DeepSeek output:", aiParsed);
    } else {
      parsed.title_ai = parsed.title;
    }

    // ---- 8ï¸âƒ£ Brave search using parsed info ----
    const braveQuery = buildBraveQueryFromParsed(parsed, joinedText);

    let braveResults = [];
    try {
      braveResults = await searchBrave(braveQuery, { count: 5 });
      console.log("ðŸ§­ Brave results (objects branch):", braveResults.length);
    } catch (e) {
      console.error("âš ï¸ Brave search failed in objects branch:", e);
    }

    res.json({ detectedObject, texts, parsed, croppedBase64, braveResults });
    fs.unlinkSync(filePath);
  } catch (err) {
    console.error("âŒ Error:", err);
    res.status(500).json({ error: "Failed to analyze image." });
  }
});


// ---- START SERVER ----
const PORT = process.env.PORT || 5050;
app.listen(PORT, () =>
  console.log(`ðŸš€ Server running on http://localhost:${PORT}`)
);

export default app;
